# -*- coding: utf-8 -*-
"""Stemming_Lemmatization.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HLcfrDUI7SFEogmK8nacgVBogilDuwXz

Program to differentiate stemming and lemmatizing words
#### Madhura K R
*22/10/2021*
"""

import nltk
#nltk.download('all')

"""**Stemming**


---

Stemming usually refers to a process that chops off the ends of words in the hope of achieving goal correctly most of the time and often includes the removal of derivational affixes.

1. PorterStemmer uses Suffix Stripping to produce stems. It is known for its simplicity and speed. PorterStemmer algorithm does not follow linguistics to generate stems. This is the reason why PorterStemmer does not often generate stems that are actual English words. It does not keep a lookup table for actual stems of the word but applies algorithmic rules to generate stems. It uses the rules to decide whether it is wise to strip a suffix.

2. LancasterStemmer (Paice-Husk stemmer) is an iterative algorithm with rules saved externally.

Stemming English words
"""

from nltk.stem import PorterStemmer

stemmer = PorterStemmer()

print (stemmer.stem('working')) # output: work
print (stemmer.stem('works')) # output: work
print (stemmer.stem('worked')) # output: work

#Stemming text 
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import LancasterStemmer
text = "Noticing these things, I rode over a short causeway to the house"
text = text.lower()
# tokenize text 
words = word_tokenize(text)
print (words)

stemmer = PorterStemmer()
words_stem = [stemmer.stem(word) for word in words]
print (words_stem)

"""PorterStemmer Vs Lancaster Stemmer"""

p = PorterStemmer() #general
l=LancasterStemmer() #general

words_stem = [p.stem(word) for word in words]
print (words_stem)

words_stem = [l.stem(word) for word in words]
print (words_stem)

"""Non- english Langs"""

from nltk.stem.snowball import SnowballStemmer
from nltk.stem.isri import ISRIStemmer
SpanishStemmer=SnowballStemmer("spanish") 
ItalianStemmer=SnowballStemmer("italian")
ArabicStemmer = ISRIStemmer()

"""**1) Spanish**

corriendo = running

corr = run
"""

SpanishStemmer.stem("corriendo")

p.stem("corriendo")

l.stem("corriendo")

"""**2) Italian**

case = houses

cas = house
"""

ItalianStemmer.stem("case")

p.stem("case")

l.stem("case")

"""**3) Arabic**

حركات = movements

حرك = move
"""

ArabicStemmer.stem("حركات")

p.stem("حركات")

l.stem("حركات")

"""**Lemmatization**



---

1. **Wordnet Lemmatizer with NLTK**
"""

import nltk
nltk.download('wordnet')

from nltk.stem import WordNetLemmatizer 
lemmatizer = WordNetLemmatizer()

print(lemmatizer.lemmatize("bats"))

"""2. **Wordnet Lemmatizer with appropriate POS tag**"""

print(lemmatizer.lemmatize("stripes", 'v'))

print(lemmatizer.lemmatize("stripes", 'n'))

"""It may not be possible manually provide the corrent POS tag for every word for large texts. So, instead, we will find out the correct POS tag for each word, map it to the right input character that the WordnetLemmatizer accepts and pass it as the second argument to lemmatize()."""

nltk.download('averaged_perceptron_tagger')

from nltk.tag import pos_tag

print(nltk.pos_tag(['feet']))

"""nltk.pos_tag() returns a tuple with the POS tag. The key here is to map NLTK’s POS tags to the format wordnet lemmatizer would accept. The get_wordnet_pos() function defined below does this mapping job."""

# Lemmatize with POS Tag


def get_wordnet_pos(word):
    """Map POS tag to first character lemmatize() accepts"""
    tag = nltk.pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": wordnet.ADJ,
                "N": wordnet.NOUN,
                "V": wordnet.VERB,
                "R": wordnet.ADV}

    return tag_dict.get(tag, wordnet.NOUN)

# 1. Init Lemmatizer
lemmatizer = WordNetLemmatizer()

# 2. Lemmatize Single Word with the appropriate POS tag
word = 'feet'
print(lemmatizer.lemmatize(word, get_wordnet_pos(word)))

"""3. **TextBlob Lemmatizer**

TexxtBlob is a powerful, fast and convenient NLP package as well. Using the Word and TextBlob objects, its quite straighforward to parse and lemmatize words and sentences respectively.
"""

!pip install textblob
from textblob import TextBlob, Word

# Lemmatize a word
word = 'stripes'
w = Word(word)
w.lemmatize()
#> stripe

"""It did not do a great job at the outset, because, like NLTK, TextBlob also uses wordnet internally. So, let’s pass the appropriate POS tag to the lemmatize() method.

4. **TextBlob Lemmatizer with appropriate POS tag**
"""

# Define function to lemmatize each word with its POS tag
def lemmatize_with_postag(sentence):
    sent = TextBlob(sentence)
    tag_dict = {"J": 'a', 
                "N": 'n', 
                "V": 'v', 
                "R": 'r'}
    words_and_tags = [(w, tag_dict.get(pos[0], 'n')) for w, pos in sent.tags]    
    lemmatized_list = [wd.lemmatize(tag) for wd, tag in words_and_tags]
    return " ".join(lemmatized_list)

"""5. **Pattern Lemmatizer**"""

#!pip install pattern

import pattern
from pattern.en import lemma, lexeme

sentence = "Noticing these things, I rode over a short causeway to the house. A servant in waiting took my horse, and I entered the Gothic archway of the hall."

# Lexeme's for each word 
[lexeme(wd) for wd in sentence.split()]

" ".join([lemma(wd) for wd in sentence.split()])

"""***DIFFERENCE:***


---

1. ‘Caring’ -> Lemmatization -> ‘Care’
2. ‘Caring’ -> Stemming -> ‘Car’

The stemming and lemmatization process are hand-written regex rules written find the root word.
"""

from nltk.stem import PorterStemmer
porter = PorterStemmer()

for word in ['walking', 'walks', 'walked']:
    print(porter.stem(word))

from nltk.stem import WordNetLemmatizer
wnl = WordNetLemmatizer()

for word in ['walking', 'walks', 'walked']:
    print(wnl.lemmatize(word))

from nltk import pos_tag
walking_tagged = pos_tag(['walking','walks', 'walked'])
print(walking_tagged)

from nltk.corpus import wordnet
def get_wordnet_pos(word):
    """Map POS tag to first character lemmatize() accepts"""
    tag = nltk.pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": wordnet.ADJ,
                "N": wordnet.NOUN,
                "V": wordnet.VERB,
                "R": wordnet.ADV}

    return tag_dict.get(tag, wordnet.NOUN)

lemmatizer = WordNetLemmatizer()
for i in ['walking','walks', 'walked']:
  print(lemmatizer.lemmatize(i, get_wordnet_pos(i)))

"""**INFERENCES**


---
<h2> Stemming </h2>

> 1. **Clearly Snowball Stemmer stems it to a more accurate stem.**
2. **Porter Stemmer is an old stemming algorithm. But with some issues in it, a better version of it was procured which is "Snowball" stemmer.**
3. **The stems obtained by the Snowball Stemmer was the most accurate among the other stemmers.**
4. **Port Stemmer was relatively inefficient in determining the stems.**
5. **However, for the English language, Porter stemmer performed better than Lancaster stemmer.**

<h2> Lemmatization</h2>

> 1. **Just the Wordnet Lemmatizer with NLTK did not do a good job in lemmatizing. (The reason given in 2nd point.)**
2. **‘part-of-speech’ tag (POS tag) as an argument to the lemmatizing function improves the results to a great extent.**
3. **The drawback of using *Wordnet Lemmatizer with appropriate POS tag* is that it may not be possible manually provide the corrent POS tag for every word for large texts.**
4. **So for the *Wordnet Lemmatizer with appropriate POS tag*, we use pos_tag() to get the POS of the token, convert it into a format acceptable by the function lemmatize() and thus obtain the lemma.**
5. **The same conclusions apply to the TextBlob, which uses Wordnet internally. The lemmatizer works better when given the POS.**
6. **For the given sentence, Pattern Lemmatizer did a better job at lemmatizing.**

<h2> Difference</h2>

> * **Stemming tries to shorten a word with simple regex rules**
* **Lemmatization Tries to find the root word with linguistics rules (with the use of regexes)**
"""